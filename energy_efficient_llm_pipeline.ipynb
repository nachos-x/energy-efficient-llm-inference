{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNW77bll+cptedQGzt9XrE1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuIX_RKRjoCd",
        "outputId": "fe2c66fd-0989-48b8-a97f-a4373853b70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pinyin (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.60.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstallation finished. If you see bitsandbytes warnings → restart runtime now.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install core packages\n",
        "\n",
        "!pip install -q --upgrade pip\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install -q -U bitsandbytes accelerate\n",
        "\n",
        "\n",
        "!pip install -q transformers datasets evaluate\n",
        "\n",
        "!pip install -q optimum[onnxruntime]\n",
        "\n",
        "!pip install -q textattack\n",
        "\n",
        "!pip install -q codecarbon mlflow\n",
        "\n",
        "!pip install -q scikit-learn python-multipart fastapi uvicorn\n",
        "\n",
        "print(\"Installation finished. If you see bitsandbytes warnings → restart runtime now.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir -p /content/energy-efficient-llm-pipeline/src\n",
        "%cd /content/energy-efficient-llm-pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmkYPajIkNF_",
        "outputId": "c35b9cf2-8e34-483a-ccb4-fd597aa1dc26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/energy-efficient-llm-pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/__init__.py\n",
        "# empty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4J2T91xkRnE",
        "outputId": "f8e4a27c-968c-4ce2-f0f5-ae26ab71e5c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/data.py\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_and_preprocess_glue():\n",
        "    dataset = load_dataset(\"glue\", \"sst2\")\n",
        "    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "    test_ds = dataset[\"validation\"].select(range(500))\n",
        "    return train_ds, test_ds\n",
        "\n",
        "def preprocess_function(examples, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=max_length\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR9DjojclqF7",
        "outputId": "af6fdf9c-31ea-4bb9-bf06-58e25c404ae5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/adversarial.py\n",
        "import textattack\n",
        "from textattack import Attacker\n",
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.attack_results import SuccessfulAttackResult\n",
        "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
        "from datasets import Dataset, concatenate_datasets, Features, Value, ClassLabel\n",
        "\n",
        "def generate_adversarial_examples(model, tokenizer, dataset, num_examples=800):\n",
        "    model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "    # Strong attack: TextFoolerJin2019\n",
        "    attack = TextFoolerJin2019.build(model_wrapper)\n",
        "\n",
        "    # Relax USE threshold for more successes\n",
        "    for constraint in attack.constraints:\n",
        "        if isinstance(constraint, UniversalSentenceEncoder):\n",
        "            constraint.threshold = 0.70\n",
        "            print(\"Relaxed USE similarity threshold to 0.70 for higher success rate\")\n",
        "\n",
        "    subset = dataset.shuffle(seed=42).select(range(min(num_examples, len(dataset))))\n",
        "    hf_dataset = HuggingFaceDataset(subset, split=\"train\")\n",
        "\n",
        "    attacker = Attacker(attack, hf_dataset)\n",
        "    results = attacker.attack_dataset()\n",
        "\n",
        "    adv_texts = []\n",
        "    adv_labels = []\n",
        "    success_count = 0\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        # Correct success check for modern TextAttack\n",
        "        if isinstance(result, SuccessfulAttackResult) and result.perturbed_text() != result.original_text():\n",
        "            adv_texts.append(result.perturbed_text())\n",
        "            adv_labels.append(result.original_result.ground_truth_output)\n",
        "            success_count += 1\n",
        "            if success_count % 20 == 0:\n",
        "                print(f\"Success {success_count}: {result.perturbed_text()[:60]}...\")\n",
        "\n",
        "    print(f\"\\nGenerated {len(adv_texts)} successful adversarial examples\")\n",
        "\n",
        "    features = Features({\n",
        "        \"sentence\": Value(\"string\"),\n",
        "        \"label\": ClassLabel(names=[\"negative\", \"positive\"])\n",
        "    })\n",
        "\n",
        "    return Dataset.from_dict({\"sentence\": adv_texts, \"label\": adv_labels}, features=features)\n",
        "\n",
        "\n",
        "def create_mixed_dataset(original_ds, adv_ds, adv_ratio=0.4):\n",
        "    if len(adv_ds) == 0:\n",
        "        print(\"No adversarial examples generated → using original dataset only\")\n",
        "        return original_ds\n",
        "\n",
        "    num_adv = int(len(original_ds) * adv_ratio)\n",
        "    original_subset = original_ds.select(range(len(original_ds) - num_adv))\n",
        "\n",
        "    mixed = concatenate_datasets([original_subset, adv_ds])\n",
        "    mixed = mixed.shuffle(seed=42)\n",
        "\n",
        "    print(f\"Mixed dataset size: {len(mixed)} (original: {len(original_subset)}, adv: {len(adv_ds)})\")\n",
        "    return mixed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znRrnslNltfB",
        "outputId": "f3fb578e-31f4-4365-c6a9-a484b70fd29c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/adversarial.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/model.py\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "\n",
        "def load_baseline_model(model_dir=\"distilbert-finetuned-sst2\"):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_quantized_bitsandbytes(model_dir):\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_dir,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "def create_pipeline(model, tokenizer):\n",
        "    return pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=32,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvgm1OZ0lx8w",
        "outputId": "a68f61e1-ae0d-4c1d-c0cb-388a97d7f17a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train.py\n",
        "import sys\n",
        "import os\n",
        "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "import mlflow\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "from evaluate import load\n",
        "import torch\n",
        "from codecarbon import OfflineEmissionsTracker\n",
        "from src.data import load_and_preprocess_glue, preprocess_function\n",
        "from src.adversarial import generate_adversarial_examples, create_mixed_dataset\n",
        "from src.model import load_baseline_model\n",
        "\n",
        "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "mlflow.set_experiment(\"energy-efficient-distilbert-sst2\")\n",
        "\n",
        "accuracy_metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "train_ds, test_ds = load_and_preprocess_glue()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "tokenized_train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer), batched=True)\n",
        "tokenized_test  = test_ds.map(lambda ex: preprocess_function(ex, tokenizer), batched=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "with mlflow.start_run(run_name=\"baseline_finetune\"):\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    tracker = OfflineEmissionsTracker(project_name=\"baseline_train\", log_level=\"error\")\n",
        "    tracker.start()\n",
        "    trainer.train()\n",
        "    co2_kg = tracker.stop()\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    mlflow.log_params({\n",
        "        \"epochs\": 3,\n",
        "        \"batch_size\": 16,\n",
        "        \"task\": \"sst2\",\n",
        "        \"model\": \"distilbert-base-uncased\"\n",
        "    })\n",
        "    mlflow.log_metric(\"final_accuracy\", eval_results[\"eval_accuracy\"])\n",
        "    mlflow.log_metric(\"co2_kg\", co2_kg)\n",
        "    mlflow.log_metric(\"approx_energy_kwh\", co2_kg / 0.35)\n",
        "\n",
        "    trainer.save_model(\"./distilbert-finetuned-sst2\")\n",
        "    tokenizer.save_pretrained(\"./distilbert-finetuned-sst2\")\n",
        "\n",
        "# Adversarial training\n",
        "print(\"Generating adversarial examples...\")\n",
        "adv_ds = generate_adversarial_examples(model, tokenizer, train_ds, num_examples=800)\n",
        "mixed_ds = create_mixed_dataset(train_ds, adv_ds, adv_ratio=0.4)\n",
        "tokenized_mixed = mixed_ds.map(lambda ex: preprocess_function(ex, tokenizer), batched=True)\n",
        "\n",
        "with mlflow.start_run(run_name=\"adversarial_finetune\"):\n",
        "    training_args.num_train_epochs = 2\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_mixed,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    tracker = OfflineEmissionsTracker(project_name=\"adv_train\", log_level=\"error\")\n",
        "    tracker.start()\n",
        "    trainer.train()\n",
        "    co2_kg_adv = tracker.stop()\n",
        "\n",
        "    eval_results_adv = trainer.evaluate()\n",
        "    mlflow.log_metric(\"final_accuracy_adv\", eval_results_adv[\"eval_accuracy\"])\n",
        "    mlflow.log_metric(\"co2_kg_adv\", co2_kg_adv)\n",
        "    mlflow.log_metric(\"approx_energy_kwh_adv\", co2_kg_adv / 0.35)\n",
        "\n",
        "    trainer.save_model(\"./distilbert-robust-sst2\")\n",
        "    tokenizer.save_pretrained(\"./distilbert-robust-sst2\")\n",
        "\n",
        "print(\"Training completed. Models saved to:\")\n",
        "print(\"  - Baseline:   ./distilbert-finetuned-sst2\")\n",
        "print(\"  - Robust:     ./distilbert-robust-sst2\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntMpJ85Jl2Hq",
        "outputId": "ec1534d6-1754-463b-b635-0a29db96450c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/inference_eval.py\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Fix import path for Colab\n",
        "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import mlflow\n",
        "from codecarbon import OfflineEmissionsTracker\n",
        "from transformers import AutoTokenizer\n",
        "from src.data import load_and_preprocess_glue, preprocess_function\n",
        "from src.model import load_quantized_bitsandbytes, create_pipeline, load_baseline_model\n",
        "\n",
        "\n",
        "\n",
        "# Load test set (500 samples from GLUE validation)\n",
        "_, test_ds = load_and_preprocess_glue()\n",
        "\n",
        "model_dir = \"./distilbert-robust-sst2\"  # use robust model\n",
        "\n",
        "# Baseline model\n",
        "baseline_model, tokenizer = load_baseline_model(model_dir)\n",
        "baseline_pipe = create_pipeline(baseline_model, tokenizer)\n",
        "\n",
        "# Quantized model (8-bit by default – see note below for 4-bit)\n",
        "quant_model, _ = load_quantized_bitsandbytes(model_dir)\n",
        "quant_pipe = create_pipeline(quant_model, tokenizer)\n",
        "\n",
        "def measure_latency(model, tokenizer, texts, runs=50, batch_size=128):\n",
        "    \"\"\"\n",
        "    Manual batched forward pass – most accurate for throughput comparison\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    times = []\n",
        "    for _ in range(runs):\n",
        "        start = time.perf_counter()\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128\n",
        "            ).to(device)\n",
        "            with torch.no_grad():\n",
        "                _ = model(**inputs)\n",
        "        times.append(time.perf_counter() - start)\n",
        "    avg_time_total = sum(times) / runs\n",
        "    return avg_time_total / len(texts)  # seconds per sample\n",
        "\n",
        "# Use a large sample for stable measurement\n",
        "sample_texts = list(test_ds[\"sentence\"][:400])  # or full len(test_ds) if you want max accuracy\n",
        "\n",
        "baseline_lat = measure_latency(baseline_model, tokenizer, sample_texts)\n",
        "quant_lat    = measure_latency(quant_model, tokenizer, sample_texts)\n",
        "\n",
        "print(f\"Baseline latency: {baseline_lat:.4f} s/sample | 8-bit: {quant_lat:.4f} s/sample\")\n",
        "print(f\"Latency reduction: {((baseline_lat - quant_lat) / baseline_lat) * 100:.1f}%\")\n",
        "\n",
        "\n",
        "num_loops = 100\n",
        "batch_size_energy = 64\n",
        "\n",
        "device_baseline = next(baseline_model.parameters()).device\n",
        "device_quant = next(quant_model.parameters()).device\n",
        "\n",
        "# Baseline energy\n",
        "tracker = OfflineEmissionsTracker(project_name=\"baseline_inference\")\n",
        "tracker.start()\n",
        "for _ in range(num_loops):\n",
        "    inputs = tokenizer(\n",
        "        sample_texts[:batch_size_energy],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device_baseline)\n",
        "    with torch.no_grad():\n",
        "        _ = baseline_model(**inputs)\n",
        "baseline_emissions = tracker.stop()\n",
        "\n",
        "# Quantized energy\n",
        "tracker = OfflineEmissionsTracker(project_name=\"quantized_inference\")\n",
        "tracker.start()\n",
        "for _ in range(num_loops):\n",
        "    inputs = tokenizer(\n",
        "        sample_texts[:batch_size_energy],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device_quant)\n",
        "    with torch.no_grad():\n",
        "        _ = quant_model(**inputs)\n",
        "emissions = tracker.stop()\n",
        "\n",
        "print(f\"\\nBaseline energy: {baseline_emissions:.6f} kg CO₂eq\")\n",
        "print(f\"Quantized energy: {emissions:.6f} kg CO₂eq\")\n",
        "if baseline_emissions > 0:\n",
        "    reduction = ((baseline_emissions - emissions) / baseline_emissions) * 100\n",
        "    print(f\"Energy reduction: {reduction:.1f}%\")\n",
        "\n",
        "print(\"\\nEvaluation finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjYWT0hrl60j",
        "outputId": "af6c8dc5-6cb7-473b-f230-682057da6658"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/inference_eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/app.py\n",
        "from fastapi import FastAPI, HTTPException, Security, Depends\n",
        "from fastapi.security import APIKeyHeader\n",
        "from pydantic import BaseModel\n",
        "from src.model import load_quantized_bitsandbytes, create_pipeline\n",
        "\n",
        "app = FastAPI(title=\"Energy-Efficient DistilBERT Inference\")\n",
        "\n",
        "# Basic API key security\n",
        "API_KEY = \"test123\"\n",
        "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
        "\n",
        "async def get_api_key(api_key: str = Security(api_key_header)):\n",
        "    if api_key != API_KEY:\n",
        "        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
        "    return api_key\n",
        "\n",
        "class PredictRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Load quantized robust model at startup\n",
        "model, tokenizer = load_quantized_bitsandbytes(\"./distilbert-robust-sst2\")\n",
        "pipe = create_pipeline(model, tokenizer)\n",
        "\n",
        "@app.post(\"/predict\", dependencies=[Depends(get_api_key)])\n",
        "async def predict(request: PredictRequest):\n",
        "    result = pipe(request.text)[0]\n",
        "    return {\n",
        "        \"label\": result[\"label\"],\n",
        "        \"score\": float(result[\"score\"]),\n",
        "        \"model\": \"distilbert-8bit-quantized-robust\"\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"healthy\", \"model_loaded\": True}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFz00VxLl-ay",
        "outputId": "43f89af2-ac24-434b-cec5-89db1b07d276"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyCKTFOClHzZ",
        "outputId": "9122caf9-5de3-4c60-b67a-cdbc418d3366"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-03 19:51:19.471985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770148279.494351    6619 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770148279.501691    6619 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770148279.526586    6619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770148279.526619    6619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770148279.526628    6619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770148279.526633    6619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 19:51:19.532273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/content/energy-efficient-llm-pipeline/src/train.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "[codecarbon INFO @ 19:51:34] offline tracker init\n",
            "[codecarbon WARNING @ 19:51:34] Multiple instances of codecarbon are allowed to run at the same time.\n",
            " 33% 63/189 [00:04<00:06, 18.28it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:00, 48.25it/s]\u001b[A\n",
            " 62% 10/16 [00:00<00:00, 41.37it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.36272862553596497, 'eval_accuracy': 0.86, 'eval_runtime': 0.4327, 'eval_samples_per_second': 1155.503, 'eval_steps_per_second': 36.976, 'epoch': 1.0}\n",
            " 33% 63/189 [00:04<00:06, 18.28it/s]\n",
            "100% 16/16 [00:00<00:00, 40.20it/s]\u001b[A\n",
            " 66% 125/189 [00:15<00:03, 17.20it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:00, 47.74it/s]\u001b[A\n",
            " 62% 10/16 [00:00<00:00, 41.01it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.4576669931411743, 'eval_accuracy': 0.832, 'eval_runtime': 0.4287, 'eval_samples_per_second': 1166.328, 'eval_steps_per_second': 37.322, 'epoch': 2.0}\n",
            " 67% 126/189 [00:15<00:03, 17.20it/s]\n",
            "100% 16/16 [00:00<00:00, 39.56it/s]\u001b[A\n",
            " 99% 187/189 [00:22<00:00, 17.47it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:00, 46.21it/s]\u001b[A\n",
            " 62% 10/16 [00:00<00:00, 41.03it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.4705187678337097, 'eval_accuracy': 0.854, 'eval_runtime': 0.4299, 'eval_samples_per_second': 1162.976, 'eval_steps_per_second': 37.215, 'epoch': 3.0}\n",
            "100% 189/189 [00:23<00:00, 17.47it/s]\n",
            "100% 16/16 [00:00<00:00, 39.55it/s]\u001b[A\n",
            "{'train_runtime': 35.9937, 'train_samples_per_second': 83.348, 'train_steps_per_second': 5.251, 'train_loss': 0.27675826461226855, 'epoch': 3.0}\n",
            "100% 189/189 [00:35<00:00,  5.25it/s]\n",
            "100% 16/16 [00:00<00:00, 34.69it/s]\n",
            "Generating adversarial examples...\n",
            "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "Relaxed USE similarity threshold to 0.70 for higher success rate\n",
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  WordSwapEmbedding(\n",
            "    (max_candidates):  50\n",
            "    (embedding):  WordEmbedding\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): WordEmbeddingDistance(\n",
            "        (embedding):  WordEmbedding\n",
            "        (min_cos_sim):  0.5\n",
            "        (cased):  False\n",
            "        (include_unknown_words):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): PartOfSpeech(\n",
            "        (tagger_type):  nltk\n",
            "        (tagset):  universal\n",
            "        (allow_verb_noun_swap):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (2): UniversalSentenceEncoder(\n",
            "        (metric):  angular\n",
            "        (threshold):  0.7\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  False\n",
            "      )\n",
            "    (3): RepeatModification\n",
            "    (4): StopwordModification\n",
            "    (5): InputColumnModification(\n",
            "        (matching_column_labels):  ['premise', 'hypothesis']\n",
            "        (columns_to_ignore):  {'premise'}\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s]2026-02-03 19:52:21.096252: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1770148341.096416    6619 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12460 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            " 10% 1/10 [00:13<02:04, 13.82s/it]--------------------------------------------- Result 1 ---------------------------------------------\n",
            "\u001b[91mNegative (87%)\u001b[0m --> \u001b[92mPositive (58%)\u001b[0m\n",
            "\n",
            "\u001b[91mlost\u001b[0m in the \u001b[91mtranslation\u001b[0m ... another \u001b[91mroutine\u001b[0m hollywood frightfest in which the \u001b[91mslack\u001b[0m \u001b[91mexecution\u001b[0m italicizes the absurdity of the \u001b[91mpremise\u001b[0m \n",
            "\n",
            "\u001b[92munaccounted\u001b[0m in the \u001b[92mtransforming\u001b[0m ... another \u001b[92mclassic\u001b[0m hollywood frightfest in which the \u001b[92mrelaxed\u001b[0m \u001b[92mfulfil\u001b[0m italicizes the absurdity of the \u001b[92masuncion\u001b[0m \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  20% 2/10 [00:14<00:56,  7.09s/it]--------------------------------------------- Result 2 ---------------------------------------------\n",
            "\u001b[92mPositive (94%)\u001b[0m --> \u001b[91mNegative (81%)\u001b[0m\n",
            "\n",
            "quite \u001b[92mtouching\u001b[0m \n",
            "\n",
            "quite \u001b[91mplaguing\u001b[0m \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  30% 3/10 [00:14<00:33,  4.80s/it]--------------------------------------------- Result 3 ---------------------------------------------\n",
            "\u001b[91mNegative (77%)\u001b[0m --> \u001b[92mPositive (93%)\u001b[0m\n",
            "\n",
            "narrative \u001b[91mbanality\u001b[0m \n",
            "\n",
            "narrative \u001b[92mcallousness\u001b[0m \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  40% 4/10 [00:14<00:21,  3.63s/it]--------------------------------------------- Result 4 ---------------------------------------------\n",
            "\u001b[92mPositive (94%)\u001b[0m --> \u001b[91mNegative (87%)\u001b[0m\n",
            "\n",
            "very \u001b[92mcapable\u001b[0m \n",
            "\n",
            "very \u001b[91mincapable\u001b[0m \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  50% 5/10 [00:14<00:14,  2.95s/it]--------------------------------------------- Result 5 ---------------------------------------------\n",
            "\u001b[91mNegative (77%)\u001b[0m --> \u001b[91m[FAILED]\u001b[0m\n",
            "\n",
            "'s nothing remotely topical or sexy here . \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 4 / 1 / 0 / 5:  60% 6/10 [00:14<00:09,  2.49s/it]--------------------------------------------- Result 6 ---------------------------------------------\n",
            "\u001b[92mPositive (90%)\u001b[0m --> \u001b[91mNegative (89%)\u001b[0m\n",
            "\n",
            "\u001b[92mperfectly\u001b[0m pleasant if \n",
            "\n",
            "\u001b[91mtoo\u001b[0m pleasant if \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 5 / 1 / 0 / 6:  70% 7/10 [00:15<00:06,  2.19s/it]--------------------------------------------- Result 7 ---------------------------------------------\n",
            "\u001b[91mNegative (82%)\u001b[0m --> \u001b[92mPositive (57%)\u001b[0m\n",
            "\n",
            "\u001b[91mdoes\u001b[0m \u001b[91mleblanc\u001b[0m \u001b[91mmake\u001b[0m one spectacularly ugly-looking \u001b[91mbroad\u001b[0m \n",
            "\n",
            "\u001b[92mwishes\u001b[0m \u001b[92mbergeron\u001b[0m \u001b[92mdeliver\u001b[0m one spectacularly ugly-looking \u001b[92mgrand\u001b[0m \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 6 / 1 / 0 / 7:  70% 7/10 [00:15<00:06,  2.20s/it]--------------------------------------------- Result 8 ---------------------------------------------\n",
            "\u001b[92mPositive (91%)\u001b[0m --> \u001b[91mNegative (84%)\u001b[0m\n",
            "\n",
            "\u001b[92madventure\u001b[0m movie \n",
            "\n",
            "\u001b[91mmisadventures\u001b[0m movie \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 7 / 1 / 0 / 8:  90% 9/10 [00:15<00:01,  1.73s/it]--------------------------------------------- Result 9 ---------------------------------------------\n",
            "\u001b[92mPositive (92%)\u001b[0m --> \u001b[91mNegative (57%)\u001b[0m\n",
            "\n",
            "rendered with such \u001b[92mclarity\u001b[0m that it 's as if it all happened only yesterday \n",
            "\n",
            "rendered with such \u001b[91mclarify\u001b[0m that it 's as if it all happened only yesterday \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 8 / 1 / 0 / 9:  90% 9/10 [00:15<00:01,  1.73s/it]--------------------------------------------- Result 10 ---------------------------------------------\n",
            "\u001b[91mNegative (58%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "can not help but love cinema paradiso , whether the original version or new director 's cut . \n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 8 / 1 / 1 / 10: 100% 10/10 [00:15<00:00,  1.56s/it]\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 8      |\n",
            "| Number of failed attacks:     | 1      |\n",
            "| Number of skipped attacks:    | 1      |\n",
            "| Original accuracy:            | 90.0%  |\n",
            "| Accuracy under attack:        | 10.0%  |\n",
            "| Attack success rate:          | 88.89% |\n",
            "| Average perturbed word %:     | 41.15% |\n",
            "| Average num. words per input: | 7.4    |\n",
            "| Avg num queries:              | 60.44  |\n",
            "+-------------------------------+--------+\n",
            "\n",
            "Generated 8 successful adversarial examples\n",
            "Mixed dataset size: 608 (original: 600, adv: 8)\n",
            "Map: 100% 608/608 [00:00<00:00, 9058.62 examples/s]\n",
            "/content/energy-efficient-llm-pipeline/src/train.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            " 50% 38/76 [00:02<00:02, 17.29it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:00, 46.05it/s]\u001b[A\n",
            " 62% 10/16 [00:00<00:00, 40.05it/s]\u001b[A\n",
            " 94% 15/16 [00:00<00:00, 39.08it/s]\u001b[A\n",
            "{'eval_loss': 0.4984351396560669, 'eval_accuracy': 0.814, 'eval_runtime': 0.4359, 'eval_samples_per_second': 1147.166, 'eval_steps_per_second': 36.709, 'epoch': 1.0}\n",
            "\n",
            " 50% 38/76 [00:02<00:02, 17.29it/s]\n",
            " 99% 75/76 [00:20<00:00, 16.17it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:00, 46.19it/s]\u001b[A\n",
            " 62% 10/16 [00:00<00:00, 39.40it/s]\u001b[A\n",
            " 94% 15/16 [00:00<00:00, 38.54it/s]\u001b[A\n",
            "{'eval_loss': 0.4432199001312256, 'eval_accuracy': 0.838, 'eval_runtime': 0.4425, 'eval_samples_per_second': 1130.017, 'eval_steps_per_second': 36.161, 'epoch': 2.0}\n",
            "\n",
            "100% 76/76 [00:20<00:00, 16.17it/s]\n",
            "{'train_runtime': 46.4081, 'train_samples_per_second': 26.202, 'train_steps_per_second': 1.638, 'train_loss': 0.2279576251381322, 'epoch': 2.0}\n",
            "100% 76/76 [00:46<00:00,  1.64it/s]\n",
            "100% 16/16 [00:00<00:00, 34.39it/s]\n",
            "Training completed. Models saved to:\n",
            "  - Baseline:   ./distilbert-finetuned-sst2\n",
            "  - Robust:     ./distilbert-robust-sst2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/inference_eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5732gkpquCC3",
        "outputId": "dec52cb1-6a5e-40c3-f686-70f9076f53a0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-03 19:55:08.138482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770148508.158099    7778 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770148508.164122    7778 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770148508.179323    7778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770148508.179346    7778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770148508.179353    7778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770148508.179356    7778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 19:55:08.184054: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Baseline latency: 0.0013 s/sample | 8-bit: 0.0003 s/sample\n",
            "Latency reduction: 74.0%\n",
            "[codecarbon INFO @ 19:55:54] offline tracker init\n",
            "[codecarbon WARNING @ 19:55:54] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 19:55:54] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 19:55:54] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 19:55:55] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 19:55:55] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 19:55:55] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 19:55:55] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 19:55:55] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 19:55:55] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 19:55:55] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 19:55:55] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 19:55:55]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 19:55:55]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 19:55:55]   CodeCarbon version: 3.2.2\n",
            "[codecarbon INFO @ 19:55:55]   Available RAM : 12.671 GB\n",
            "[codecarbon INFO @ 19:55:55]   CPU count: 2 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 19:55:55]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 19:55:55]   GPU count: 1\n",
            "[codecarbon INFO @ 19:55:55]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 19:55:55] Emissions data (if any) will be saved to file /content/energy-efficient-llm-pipeline/emissions.csv\n",
            "[codecarbon INFO @ 19:56:02] Energy consumed for RAM : 0.000020 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 19:56:02] Delta energy consumed for CPU with constant : 0.000086 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 19:56:02] Energy consumed for All CPU : 0.000086 kWh\n",
            "[codecarbon INFO @ 19:56:02] Energy consumed for all GPUs : 0.000142 kWh. Total GPU Power : 69.79450467359439 W\n",
            "[codecarbon INFO @ 19:56:02] 0.000248 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon WARNING @ 19:56:02] We do not have data for None, using world average.\n",
            "[codecarbon WARNING @ 19:56:02] The CSV format has changed, backing up old emission file.\n",
            "[codecarbon INFO @ 19:56:02] offline tracker init\n",
            "[codecarbon WARNING @ 19:56:02] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 19:56:02] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 19:56:02] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 19:56:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 19:56:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 19:56:02] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 19:56:02] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 19:56:02] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 19:56:02] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 19:56:02] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 19:56:02] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 19:56:02]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 19:56:02]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 19:56:02]   CodeCarbon version: 3.2.2\n",
            "[codecarbon INFO @ 19:56:02]   Available RAM : 12.671 GB\n",
            "[codecarbon INFO @ 19:56:02]   CPU count: 2 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 19:56:02]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 19:56:02]   GPU count: 1\n",
            "[codecarbon INFO @ 19:56:02]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 19:56:02] Emissions data (if any) will be saved to file /content/energy-efficient-llm-pipeline/emissions.csv\n",
            "[codecarbon INFO @ 19:56:05] Energy consumed for RAM : 0.000007 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 19:56:05] Delta energy consumed for CPU with constant : 0.000030 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 19:56:05] Energy consumed for All CPU : 0.000030 kWh\n",
            "[codecarbon INFO @ 19:56:05] Energy consumed for all GPUs : 0.000049 kWh. Total GPU Power : 68.61033844305948 W\n",
            "[codecarbon INFO @ 19:56:05] 0.000087 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
            "[codecarbon WARNING @ 19:56:05] We do not have data for None, using world average.\n",
            "\n",
            "Baseline energy: 0.000118 kg CO₂eq\n",
            "Quantized energy: 0.000041 kg CO₂eq\n",
            "Energy reduction: 65.0%\n",
            "\n",
            "Evaluation finished.\n"
          ]
        }
      ]
    }
  ]
}